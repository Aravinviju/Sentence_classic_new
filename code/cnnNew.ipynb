{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17671537360288316268\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 170459136\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 9493394957667786037\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 338231296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13832129856767949179\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "\n",
    "import gzip\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    \n",
    "    import cPickle as pkl\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import Regularizer\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordIdxLookup(word, word_idx_map):\n",
    "    if word in word_idx_map:\n",
    "        return word_idx_map[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n"
     ]
    }
   ],
   "source": [
    "data = pkl.load(gzip.open(\"/home/dl1/Arav/Sentence_classic/code/pkl/data.pkl.gz\",\"rb\"))\n",
    "print(\"data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = data['train']['labels']\n",
    "train_sentences = data['train']['sentences']\n",
    "\n",
    "dev_labels = data['dev']['labels']\n",
    "dev_sentences = data['dev']['sentences']\n",
    "\n",
    "test_labels = data['test']['labels']\n",
    "test_sentences = data['test']['sentences']\n",
    "\n",
    "word_embeddings = data['wordEmbeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence: 59\n"
     ]
    }
   ],
   "source": [
    "# :: Find the longest sentence in our dataset ::\n",
    "max_sentence_len = 0\n",
    "for sentence in train_sentences + dev_sentences + test_sentences:\n",
    "    max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "\n",
    "print(\"Longest sentence: %d\" % max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5330, 59)\n",
      "X_dev shape: (2664, 59)\n",
      "X_test shape: (2668, 59)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_dev = np.array(dev_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "X_train = sequence.pad_sequences(train_sentences, maxlen=max_sentence_len)\n",
    "X_dev = sequence.pad_sequences(dev_sentences, maxlen=max_sentence_len)\n",
    "X_test = sequence.pad_sequences(test_sentences, maxlen=max_sentence_len)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_dev shape:', X_dev.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#  :: Create the network :: \n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 50\n",
    "\n",
    "nb_filter = 50\n",
    "filter_lengths = [1,2,3]\n",
    "hidden_dims = 100\n",
    "nb_epoch = 100\n",
    "\n",
    "\n",
    "\n",
    "words_input = Input(shape=(max_sentence_len,), dtype='int32', name='words_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'words_input_1:0' shape=(?, 59) dtype=int32>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words_input]\n",
    "# word_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our word embedding layer\n",
    "wordsEmbeddingLayer = Embedding(word_embeddings.shape[0],\n",
    "                    word_embeddings.shape[1],                                     \n",
    "                    weights=[word_embeddings],\n",
    "                    trainable=False)\n",
    "\n",
    "words = wordsEmbeddingLayer(words_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_2/Gather:0' shape=(?, 59, 300) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordsEmbeddingLayer\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_input (InputLayer)        (None, 59)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 59, 300)      4966200     words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 59, 50)       15050       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 59, 50)       30050       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 59, 50)       45050       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 50)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 50)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 50)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 150)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 150)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          15100       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            101         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,071,551\n",
      "Trainable params: 105,351\n",
      "Non-trainable params: 4,966,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Now we add a variable number of convolutions\n",
    "words_convolutions = []\n",
    "for filter_length in filter_lengths:\n",
    "    words_conv = Convolution1D(filters=nb_filter,\n",
    "                            kernel_size=filter_length,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)(words)\n",
    "                            \n",
    "    words_conv = GlobalMaxPooling1D()(words_conv)      \n",
    "    \n",
    "    words_convolutions.append(words_conv)  \n",
    "\n",
    "output = concatenate(words_convolutions)\n",
    "\n",
    "\n",
    "\n",
    "# We add a vanilla hidden layer together with dropout layers:\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(hidden_dims, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "output = Dropout(0.25)(output)\n",
    "\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "output = Dense(1, activation='sigmoid',  kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "\n",
    "model = Model(inputs=[words_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5330 samples, validate on 2664 samples\n",
      "Epoch 1/100\n",
      "5330/5330 [==============================] - 3s 500us/step - loss: 1.5313 - acc: 0.5739 - val_loss: 1.1277 - val_acc: 0.6989\n",
      "Epoch 2/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.9472 - acc: 0.7158 - val_loss: 0.7725 - val_acc: 0.7691\n",
      "Epoch 3/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.6715 - acc: 0.7829 - val_loss: 0.6215 - val_acc: 0.7785\n",
      "Epoch 4/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.5460 - acc: 0.8084 - val_loss: 0.5558 - val_acc: 0.7887\n",
      "Epoch 5/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.4638 - acc: 0.8358 - val_loss: 0.5462 - val_acc: 0.7744\n",
      "Epoch 6/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.4120 - acc: 0.8508 - val_loss: 0.5196 - val_acc: 0.7905\n",
      "Epoch 7/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.3713 - acc: 0.8775 - val_loss: 0.5193 - val_acc: 0.7860\n",
      "Epoch 8/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.3399 - acc: 0.8929 - val_loss: 0.5458 - val_acc: 0.7785\n",
      "Epoch 9/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.3067 - acc: 0.9096 - val_loss: 0.5405 - val_acc: 0.7830\n",
      "Epoch 10/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.2877 - acc: 0.9197 - val_loss: 0.5561 - val_acc: 0.7838\n",
      "Epoch 11/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.2645 - acc: 0.9321 - val_loss: 0.5511 - val_acc: 0.7920\n",
      "Epoch 12/100\n",
      "5330/5330 [==============================] - 1s 200us/step - loss: 0.2537 - acc: 0.9388 - val_loss: 0.5662 - val_acc: 0.7898\n",
      "Epoch 13/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.2420 - acc: 0.9448 - val_loss: 0.5746 - val_acc: 0.7849\n",
      "Epoch 14/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.2298 - acc: 0.9544 - val_loss: 0.5678 - val_acc: 0.7898\n",
      "Epoch 15/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.2149 - acc: 0.9602 - val_loss: 0.5876 - val_acc: 0.7838\n",
      "Epoch 16/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.2184 - acc: 0.9552 - val_loss: 0.5806 - val_acc: 0.7872\n",
      "Epoch 17/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.2075 - acc: 0.9642 - val_loss: 0.5960 - val_acc: 0.7879\n",
      "Epoch 18/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1985 - acc: 0.9687 - val_loss: 0.5917 - val_acc: 0.7943\n",
      "Epoch 19/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1926 - acc: 0.9702 - val_loss: 0.6262 - val_acc: 0.7815\n",
      "Epoch 20/100\n",
      "5330/5330 [==============================] - 1s 194us/step - loss: 0.1901 - acc: 0.9720 - val_loss: 0.6384 - val_acc: 0.7815\n",
      "Epoch 21/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1839 - acc: 0.9728 - val_loss: 0.5930 - val_acc: 0.7845\n",
      "Epoch 22/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1807 - acc: 0.9754 - val_loss: 0.5987 - val_acc: 0.7875\n",
      "Epoch 23/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1801 - acc: 0.9739 - val_loss: 0.6188 - val_acc: 0.7793\n",
      "Epoch 24/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1665 - acc: 0.9818 - val_loss: 0.6197 - val_acc: 0.7778\n",
      "Epoch 25/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1695 - acc: 0.9769 - val_loss: 0.6053 - val_acc: 0.7965\n",
      "Epoch 26/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1639 - acc: 0.9803 - val_loss: 0.5929 - val_acc: 0.7909\n",
      "Epoch 27/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1591 - acc: 0.9816 - val_loss: 0.5962 - val_acc: 0.7894\n",
      "Epoch 28/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1596 - acc: 0.9816 - val_loss: 0.6029 - val_acc: 0.7875\n",
      "Epoch 29/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.1487 - acc: 0.9857 - val_loss: 0.6063 - val_acc: 0.7932\n",
      "Epoch 30/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1509 - acc: 0.9861 - val_loss: 0.6016 - val_acc: 0.7924\n",
      "Epoch 31/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1509 - acc: 0.9833 - val_loss: 0.6041 - val_acc: 0.7928\n",
      "Epoch 32/100\n",
      "5330/5330 [==============================] - 2s 451us/step - loss: 0.1443 - acc: 0.9869 - val_loss: 0.6105 - val_acc: 0.7909\n",
      "Epoch 33/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1464 - acc: 0.9850 - val_loss: 0.6192 - val_acc: 0.7845\n",
      "Epoch 34/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1485 - acc: 0.9846 - val_loss: 0.6078 - val_acc: 0.7879\n",
      "Epoch 35/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1389 - acc: 0.9884 - val_loss: 0.6086 - val_acc: 0.7917\n",
      "Epoch 36/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1448 - acc: 0.9863 - val_loss: 0.6267 - val_acc: 0.7842\n",
      "Epoch 37/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1398 - acc: 0.9861 - val_loss: 0.6266 - val_acc: 0.7823\n",
      "Epoch 38/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1402 - acc: 0.9861 - val_loss: 0.6218 - val_acc: 0.7823\n",
      "Epoch 39/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1380 - acc: 0.9878 - val_loss: 0.6099 - val_acc: 0.7883\n",
      "Epoch 40/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1321 - acc: 0.9882 - val_loss: 0.6027 - val_acc: 0.7920\n",
      "Epoch 41/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1297 - acc: 0.9899 - val_loss: 0.6151 - val_acc: 0.7879\n",
      "Epoch 42/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.1267 - acc: 0.9912 - val_loss: 0.6122 - val_acc: 0.7939\n",
      "Epoch 43/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1280 - acc: 0.9889 - val_loss: 0.6019 - val_acc: 0.7909\n",
      "Epoch 44/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1298 - acc: 0.9884 - val_loss: 0.6009 - val_acc: 0.7913\n",
      "Epoch 45/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1243 - acc: 0.9904 - val_loss: 0.6075 - val_acc: 0.7830\n",
      "Epoch 46/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1183 - acc: 0.9925 - val_loss: 0.6193 - val_acc: 0.7875\n",
      "Epoch 47/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1257 - acc: 0.9889 - val_loss: 0.6163 - val_acc: 0.7842\n",
      "Epoch 48/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1221 - acc: 0.9902 - val_loss: 0.6230 - val_acc: 0.7860\n",
      "Epoch 49/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.1216 - acc: 0.9901 - val_loss: 0.6228 - val_acc: 0.7890\n",
      "Epoch 50/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1173 - acc: 0.9927 - val_loss: 0.6187 - val_acc: 0.7962\n",
      "Epoch 51/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1180 - acc: 0.9919 - val_loss: 0.6152 - val_acc: 0.7872\n",
      "Epoch 52/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1169 - acc: 0.9897 - val_loss: 0.6273 - val_acc: 0.7864\n",
      "Epoch 53/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1173 - acc: 0.9897 - val_loss: 0.6523 - val_acc: 0.7815\n",
      "Epoch 54/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1202 - acc: 0.9897 - val_loss: 0.6174 - val_acc: 0.7898\n",
      "Epoch 55/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1153 - acc: 0.9919 - val_loss: 0.6185 - val_acc: 0.7845\n",
      "Epoch 56/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1166 - acc: 0.9893 - val_loss: 0.6225 - val_acc: 0.7868\n",
      "Epoch 57/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1141 - acc: 0.9927 - val_loss: 0.6151 - val_acc: 0.7935\n",
      "Epoch 58/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.1108 - acc: 0.9936 - val_loss: 0.6183 - val_acc: 0.7868\n",
      "Epoch 59/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1081 - acc: 0.9949 - val_loss: 0.6579 - val_acc: 0.7834\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1122 - acc: 0.9904 - val_loss: 0.6223 - val_acc: 0.7932\n",
      "Epoch 61/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1118 - acc: 0.9919 - val_loss: 0.6167 - val_acc: 0.7890\n",
      "Epoch 62/100\n",
      "5330/5330 [==============================] - 2s 447us/step - loss: 0.1090 - acc: 0.9921 - val_loss: 0.6409 - val_acc: 0.7842\n",
      "Epoch 63/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1081 - acc: 0.9929 - val_loss: 0.6092 - val_acc: 0.7920\n",
      "Epoch 64/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1060 - acc: 0.9934 - val_loss: 0.6413 - val_acc: 0.7827\n",
      "Epoch 65/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1085 - acc: 0.9919 - val_loss: 0.6692 - val_acc: 0.7770\n",
      "Epoch 66/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1067 - acc: 0.9942 - val_loss: 0.5927 - val_acc: 0.7864\n",
      "Epoch 67/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.1033 - acc: 0.9925 - val_loss: 0.5984 - val_acc: 0.7935\n",
      "Epoch 68/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1081 - acc: 0.9921 - val_loss: 0.6097 - val_acc: 0.7924\n",
      "Epoch 69/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.0994 - acc: 0.9957 - val_loss: 0.6184 - val_acc: 0.7913\n",
      "Epoch 70/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1033 - acc: 0.9934 - val_loss: 0.6524 - val_acc: 0.7857\n",
      "Epoch 71/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1009 - acc: 0.9932 - val_loss: 0.6112 - val_acc: 0.7905\n",
      "Epoch 72/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1000 - acc: 0.9946 - val_loss: 0.6302 - val_acc: 0.7902\n",
      "Epoch 73/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1010 - acc: 0.9940 - val_loss: 0.6096 - val_acc: 0.7924\n",
      "Epoch 74/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1002 - acc: 0.9936 - val_loss: 0.6116 - val_acc: 0.7939\n",
      "Epoch 75/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1032 - acc: 0.9929 - val_loss: 0.6211 - val_acc: 0.7984\n",
      "Epoch 76/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0965 - acc: 0.9940 - val_loss: 0.6106 - val_acc: 0.7935\n",
      "Epoch 77/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0982 - acc: 0.9932 - val_loss: 0.6111 - val_acc: 0.7924\n",
      "Epoch 78/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0961 - acc: 0.9949 - val_loss: 0.6169 - val_acc: 0.7943\n",
      "Epoch 79/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0987 - acc: 0.9927 - val_loss: 0.6136 - val_acc: 0.7932\n",
      "Epoch 80/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0920 - acc: 0.9955 - val_loss: 0.6071 - val_acc: 0.7920\n",
      "Epoch 81/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0934 - acc: 0.9957 - val_loss: 0.6203 - val_acc: 0.7950\n",
      "Epoch 82/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0943 - acc: 0.9944 - val_loss: 0.6108 - val_acc: 0.7943\n",
      "Epoch 83/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0934 - acc: 0.9946 - val_loss: 0.6093 - val_acc: 0.7920\n",
      "Epoch 84/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0945 - acc: 0.9946 - val_loss: 0.6219 - val_acc: 0.7939\n",
      "Epoch 85/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0882 - acc: 0.9962 - val_loss: 0.6121 - val_acc: 0.7928\n",
      "Epoch 86/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.0892 - acc: 0.9962 - val_loss: 0.6361 - val_acc: 0.7905\n",
      "Epoch 87/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0907 - acc: 0.9947 - val_loss: 0.6259 - val_acc: 0.7898\n",
      "Epoch 88/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0899 - acc: 0.9949 - val_loss: 0.6068 - val_acc: 0.7932\n",
      "Epoch 89/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0909 - acc: 0.9955 - val_loss: 0.6130 - val_acc: 0.7879\n",
      "Epoch 90/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.0930 - acc: 0.9942 - val_loss: 0.6141 - val_acc: 0.7830\n",
      "Epoch 91/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0916 - acc: 0.9934 - val_loss: 0.6169 - val_acc: 0.7905\n",
      "Epoch 92/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.0891 - acc: 0.9953 - val_loss: 0.6150 - val_acc: 0.7913\n",
      "Epoch 93/100\n",
      "5330/5330 [==============================] - 2s 449us/step - loss: 0.0885 - acc: 0.9946 - val_loss: 0.6128 - val_acc: 0.7924\n",
      "Epoch 94/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.0854 - acc: 0.9964 - val_loss: 0.6232 - val_acc: 0.7845\n",
      "Epoch 95/100\n",
      "5330/5330 [==============================] - 1s 200us/step - loss: 0.0849 - acc: 0.9961 - val_loss: 0.6144 - val_acc: 0.7883\n",
      "Epoch 96/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0845 - acc: 0.9966 - val_loss: 0.6434 - val_acc: 0.7872\n",
      "Epoch 97/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0885 - acc: 0.9946 - val_loss: 0.6231 - val_acc: 0.7868\n",
      "Epoch 98/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0838 - acc: 0.9968 - val_loss: 0.6349 - val_acc: 0.7872\n",
      "Epoch 99/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0855 - acc: 0.9957 - val_loss: 0.6271 - val_acc: 0.7917\n",
      "Epoch 100/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0829 - acc: 0.9972 - val_loss: 0.6192 - val_acc: 0.7894\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(nb_epoch):\n",
    "# print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "\n",
    "trModel = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1,validation_data=(X_dev,y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2664/2664 [==============================] - 0s 85us/step\n",
      "2668/2668 [==============================] - 0s 84us/step\n",
      "Val-Accuracy: 78.94% (loss: 0.6192)\n",
      "Test-Accuracy: 78.15% (loss: 0.6318)\n"
     ]
    }
   ],
   "source": [
    "#Use Keras to compute the loss and the accuracy\n",
    "dev_loss, dev_accuracy = model.evaluate(X_dev, y_dev, verbose=1)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "\n",
    "print(\"Val-Accuracy: %.2f%% (loss: %.4f)\" % (dev_accuracy*100, dev_loss))\n",
    "print(\"Test-Accuracy: %.2f%% (loss: %.4f)\" % (test_accuracy*100, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Loss Curves')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Loss and Validation Loss\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(trModel.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(trModel.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accuracy Curves')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Accuracy and Validation Accuracy\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(trModel.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(trModel.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00644799], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/home/dl1/Arav/Sentence_classic/models/model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loadedModel = load_model(\"/home/dl1/Arav/Sentence_classic/models/model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n"
     ]
    }
   ],
   "source": [
    "result_data = pkl.load(gzip.open(\"/home/dl1/Arav/Sentence_classic/code/pkl/resultdata.pkl.gz\",\"rb\"))\n",
    "print(\"data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[118, 129, 931, 910, 8, 669, 9, 30, 28, 1225]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_X = sequence.pad_sequences(result_data, maxlen=max_sentence_len)\n",
    "# result_data.extend([0] * (59 - len(result_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  118,  129,  931,  910,    8,  669,\n",
       "           9,   30,   28, 1225]], dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_y_pred = loadedModel.predict(result_X, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00025697], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_y_pred.ravel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
