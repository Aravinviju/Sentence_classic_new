{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14657821713007657767\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 336134144\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 8133850262271022723\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 338231296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18229061490150543876\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "\n",
    "import gzip\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    \n",
    "    import cPickle as pkl\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import Regularizer\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordIdxLookup(word, word_idx_map):\n",
    "    if word in word_idx_map:\n",
    "        return word_idx_map[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n"
     ]
    }
   ],
   "source": [
    "data = pkl.load(gzip.open(\"/home/dl1/Arav/Sentence_classic/code/pkl/data.pkl.gz\",\"rb\"))\n",
    "print(\"data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = data['train']['labels']\n",
    "train_sentences = data['train']['sentences']\n",
    "\n",
    "dev_labels = data['dev']['labels']\n",
    "dev_sentences = data['dev']['sentences']\n",
    "\n",
    "test_labels = data['test']['labels']\n",
    "test_sentences = data['test']['sentences']\n",
    "\n",
    "word_embeddings = data['wordEmbeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence: 59\n"
     ]
    }
   ],
   "source": [
    "# :: Find the longest sentence in our dataset ::\n",
    "max_sentence_len = 0\n",
    "for sentence in train_sentences + dev_sentences + test_sentences:\n",
    "    max_sentence_len = max(len(sentence), max_sentence_len)\n",
    "\n",
    "print(\"Longest sentence: %d\" % max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5330, 59)\n",
      "X_dev shape: (2664, 59)\n",
      "X_test shape: (2668, 59)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_dev = np.array(dev_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "X_train = sequence.pad_sequences(train_sentences, maxlen=max_sentence_len)\n",
    "X_dev = sequence.pad_sequences(dev_sentences, maxlen=max_sentence_len)\n",
    "X_test = sequence.pad_sequences(test_sentences, maxlen=max_sentence_len)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_dev shape:', X_dev.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#  :: Create the network :: \n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 50\n",
    "\n",
    "nb_filter = 50\n",
    "filter_lengths = [1,2,3]\n",
    "hidden_dims = 100\n",
    "nb_epoch = 100\n",
    "\n",
    "\n",
    "\n",
    "words_input = Input(shape=(max_sentence_len,), dtype='int32', name='words_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'words_input:0' shape=(?, 59) dtype=int32>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words_input]\n",
    "# word_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our word embedding layer\n",
    "wordsEmbeddingLayer = Embedding(word_embeddings.shape[0],\n",
    "                    word_embeddings.shape[1],                                     \n",
    "                    weights=[word_embeddings],\n",
    "                    trainable=False)\n",
    "\n",
    "words = wordsEmbeddingLayer(words_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_1/Gather:0' shape=(?, 59, 300) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordsEmbeddingLayer\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_input (InputLayer)        (None, 59)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 59, 300)      4966200     words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 59, 50)       15050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 59, 50)       30050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 59, 50)       45050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 50)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 50)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 50)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 150)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          15100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            101         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,071,551\n",
      "Trainable params: 105,351\n",
      "Non-trainable params: 4,966,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Now we add a variable number of convolutions\n",
    "words_convolutions = []\n",
    "for filter_length in filter_lengths:\n",
    "    words_conv = Convolution1D(filters=nb_filter,\n",
    "                            kernel_size=filter_length,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides=1)(words)\n",
    "                            \n",
    "    words_conv = GlobalMaxPooling1D()(words_conv)      \n",
    "    \n",
    "    words_convolutions.append(words_conv)  \n",
    "\n",
    "output = concatenate(words_convolutions)\n",
    "\n",
    "\n",
    "\n",
    "# We add a vanilla hidden layer together with dropout layers:\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(hidden_dims, activation='tanh', kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "output = Dropout(0.25)(output)\n",
    "\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "output = Dense(1, activation='sigmoid',  kernel_regularizer=keras.regularizers.l2(0.01))(output)\n",
    "\n",
    "model = Model(inputs=[words_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5330 samples, validate on 2664 samples\n",
      "Epoch 1/100\n",
      "5330/5330 [==============================] - 2s 370us/step - loss: 1.5313 - acc: 0.5739 - val_loss: 1.1277 - val_acc: 0.6989\n",
      "Epoch 2/100\n",
      "5330/5330 [==============================] - 2s 447us/step - loss: 0.9472 - acc: 0.7158 - val_loss: 0.7725 - val_acc: 0.7691\n",
      "Epoch 3/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.6715 - acc: 0.7829 - val_loss: 0.6215 - val_acc: 0.7785\n",
      "Epoch 4/100\n",
      "5330/5330 [==============================] - 1s 193us/step - loss: 0.5461 - acc: 0.8092 - val_loss: 0.5559 - val_acc: 0.7890\n",
      "Epoch 5/100\n",
      "5330/5330 [==============================] - 1s 193us/step - loss: 0.4635 - acc: 0.8383 - val_loss: 0.5453 - val_acc: 0.7706\n",
      "Epoch 6/100\n",
      "5330/5330 [==============================] - 1s 194us/step - loss: 0.4126 - acc: 0.8514 - val_loss: 0.5197 - val_acc: 0.7913\n",
      "Epoch 7/100\n",
      "5330/5330 [==============================] - 1s 194us/step - loss: 0.3735 - acc: 0.8764 - val_loss: 0.5182 - val_acc: 0.7875\n",
      "Epoch 8/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.3423 - acc: 0.8874 - val_loss: 0.5343 - val_acc: 0.7819\n",
      "Epoch 9/100\n",
      "5330/5330 [==============================] - 1s 194us/step - loss: 0.3049 - acc: 0.9105 - val_loss: 0.5438 - val_acc: 0.7842\n",
      "Epoch 10/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.2890 - acc: 0.9212 - val_loss: 0.5618 - val_acc: 0.7823\n",
      "Epoch 11/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.2647 - acc: 0.9353 - val_loss: 0.5514 - val_acc: 0.7842\n",
      "Epoch 12/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.2507 - acc: 0.9417 - val_loss: 0.5607 - val_acc: 0.7894\n",
      "Epoch 13/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.2442 - acc: 0.9443 - val_loss: 0.5696 - val_acc: 0.7875\n",
      "Epoch 14/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.2280 - acc: 0.9559 - val_loss: 0.5635 - val_acc: 0.7902\n",
      "Epoch 15/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.2150 - acc: 0.9589 - val_loss: 0.5788 - val_acc: 0.7905\n",
      "Epoch 16/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.2170 - acc: 0.9559 - val_loss: 0.5816 - val_acc: 0.7860\n",
      "Epoch 17/100\n",
      "5330/5330 [==============================] - 1s 193us/step - loss: 0.2068 - acc: 0.9659 - val_loss: 0.5970 - val_acc: 0.7883\n",
      "Epoch 18/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1992 - acc: 0.9649 - val_loss: 0.5943 - val_acc: 0.7875\n",
      "Epoch 19/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1918 - acc: 0.9698 - val_loss: 0.6225 - val_acc: 0.7755\n",
      "Epoch 20/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1882 - acc: 0.9735 - val_loss: 0.6132 - val_acc: 0.7827\n",
      "Epoch 21/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1838 - acc: 0.9734 - val_loss: 0.5915 - val_acc: 0.7860\n",
      "Epoch 22/100\n",
      "5330/5330 [==============================] - 1s 194us/step - loss: 0.1813 - acc: 0.9743 - val_loss: 0.5976 - val_acc: 0.7875\n",
      "Epoch 23/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1813 - acc: 0.9711 - val_loss: 0.6013 - val_acc: 0.7815\n",
      "Epoch 24/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1699 - acc: 0.9795 - val_loss: 0.6096 - val_acc: 0.7830\n",
      "Epoch 25/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1686 - acc: 0.9765 - val_loss: 0.5996 - val_acc: 0.7905\n",
      "Epoch 26/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1651 - acc: 0.9801 - val_loss: 0.5981 - val_acc: 0.7890\n",
      "Epoch 27/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1596 - acc: 0.9814 - val_loss: 0.6181 - val_acc: 0.7778\n",
      "Epoch 28/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1603 - acc: 0.9833 - val_loss: 0.6076 - val_acc: 0.7819\n",
      "Epoch 29/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1500 - acc: 0.9856 - val_loss: 0.6128 - val_acc: 0.7887\n",
      "Epoch 30/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1484 - acc: 0.9856 - val_loss: 0.6003 - val_acc: 0.7917\n",
      "Epoch 31/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1524 - acc: 0.9837 - val_loss: 0.6029 - val_acc: 0.7894\n",
      "Epoch 32/100\n",
      "5330/5330 [==============================] - 1s 194us/step - loss: 0.1443 - acc: 0.9863 - val_loss: 0.6117 - val_acc: 0.7872\n",
      "Epoch 33/100\n",
      "5330/5330 [==============================] - 2s 446us/step - loss: 0.1468 - acc: 0.9850 - val_loss: 0.6151 - val_acc: 0.7898\n",
      "Epoch 34/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1465 - acc: 0.9863 - val_loss: 0.6063 - val_acc: 0.7868\n",
      "Epoch 35/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1390 - acc: 0.9889 - val_loss: 0.6163 - val_acc: 0.7905\n",
      "Epoch 36/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1439 - acc: 0.9857 - val_loss: 0.6364 - val_acc: 0.7774\n",
      "Epoch 37/100\n",
      "5330/5330 [==============================] - 1s 194us/step - loss: 0.1397 - acc: 0.9880 - val_loss: 0.6293 - val_acc: 0.7789\n",
      "Epoch 38/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1376 - acc: 0.9857 - val_loss: 0.6100 - val_acc: 0.7845\n",
      "Epoch 39/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1364 - acc: 0.9867 - val_loss: 0.6083 - val_acc: 0.7894\n",
      "Epoch 40/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1337 - acc: 0.9874 - val_loss: 0.6179 - val_acc: 0.7920\n",
      "Epoch 41/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1276 - acc: 0.9916 - val_loss: 0.6227 - val_acc: 0.7853\n",
      "Epoch 42/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1299 - acc: 0.9874 - val_loss: 0.6070 - val_acc: 0.7905\n",
      "Epoch 43/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1292 - acc: 0.9893 - val_loss: 0.6037 - val_acc: 0.7917\n",
      "Epoch 44/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1283 - acc: 0.9886 - val_loss: 0.6112 - val_acc: 0.7894\n",
      "Epoch 45/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1272 - acc: 0.9889 - val_loss: 0.6058 - val_acc: 0.7864\n",
      "Epoch 46/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1198 - acc: 0.9929 - val_loss: 0.6528 - val_acc: 0.7902\n",
      "Epoch 47/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1256 - acc: 0.9916 - val_loss: 0.6219 - val_acc: 0.7800\n",
      "Epoch 48/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1258 - acc: 0.9891 - val_loss: 0.6178 - val_acc: 0.7842\n",
      "Epoch 49/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1235 - acc: 0.9916 - val_loss: 0.6107 - val_acc: 0.7860\n",
      "Epoch 50/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1205 - acc: 0.9931 - val_loss: 0.6145 - val_acc: 0.7883\n",
      "Epoch 51/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1172 - acc: 0.9923 - val_loss: 0.6175 - val_acc: 0.7819\n",
      "Epoch 52/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1159 - acc: 0.9917 - val_loss: 0.6346 - val_acc: 0.7782\n",
      "Epoch 53/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1172 - acc: 0.9910 - val_loss: 0.6465 - val_acc: 0.7804\n",
      "Epoch 54/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1215 - acc: 0.9895 - val_loss: 0.6194 - val_acc: 0.7879\n",
      "Epoch 55/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1176 - acc: 0.9916 - val_loss: 0.6202 - val_acc: 0.7830\n",
      "Epoch 56/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1156 - acc: 0.9908 - val_loss: 0.6068 - val_acc: 0.7894\n",
      "Epoch 57/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1145 - acc: 0.9917 - val_loss: 0.6132 - val_acc: 0.7913\n",
      "Epoch 58/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1135 - acc: 0.9917 - val_loss: 0.6191 - val_acc: 0.7853\n",
      "Epoch 59/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1073 - acc: 0.9946 - val_loss: 0.6635 - val_acc: 0.7800\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1095 - acc: 0.9912 - val_loss: 0.6095 - val_acc: 0.7894\n",
      "Epoch 61/100\n",
      "5330/5330 [==============================] - 1s 194us/step - loss: 0.1147 - acc: 0.9906 - val_loss: 0.6152 - val_acc: 0.7909\n",
      "Epoch 62/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1094 - acc: 0.9917 - val_loss: 0.6279 - val_acc: 0.7804\n",
      "Epoch 63/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1068 - acc: 0.9929 - val_loss: 0.6054 - val_acc: 0.7898\n",
      "Epoch 64/100\n",
      "5330/5330 [==============================] - 2s 448us/step - loss: 0.1038 - acc: 0.9938 - val_loss: 0.6236 - val_acc: 0.7842\n",
      "Epoch 65/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1071 - acc: 0.9921 - val_loss: 0.6506 - val_acc: 0.7842\n",
      "Epoch 66/100\n",
      "5330/5330 [==============================] - 1s 193us/step - loss: 0.1042 - acc: 0.9946 - val_loss: 0.6009 - val_acc: 0.7890\n",
      "Epoch 67/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.1043 - acc: 0.9923 - val_loss: 0.6009 - val_acc: 0.7864\n",
      "Epoch 68/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1075 - acc: 0.9932 - val_loss: 0.6067 - val_acc: 0.7894\n",
      "Epoch 69/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.1030 - acc: 0.9931 - val_loss: 0.6211 - val_acc: 0.7838\n",
      "Epoch 70/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1045 - acc: 0.9931 - val_loss: 0.6212 - val_acc: 0.7902\n",
      "Epoch 71/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1007 - acc: 0.9951 - val_loss: 0.6049 - val_acc: 0.7834\n",
      "Epoch 72/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0999 - acc: 0.9938 - val_loss: 0.6284 - val_acc: 0.7872\n",
      "Epoch 73/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1007 - acc: 0.9949 - val_loss: 0.6108 - val_acc: 0.7849\n",
      "Epoch 74/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.1000 - acc: 0.9947 - val_loss: 0.6165 - val_acc: 0.7868\n",
      "Epoch 75/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1026 - acc: 0.9929 - val_loss: 0.6202 - val_acc: 0.7853\n",
      "Epoch 76/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0987 - acc: 0.9949 - val_loss: 0.6253 - val_acc: 0.7872\n",
      "Epoch 77/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.1007 - acc: 0.9929 - val_loss: 0.6192 - val_acc: 0.7879\n",
      "Epoch 78/100\n",
      "5330/5330 [==============================] - 1s 195us/step - loss: 0.0964 - acc: 0.9944 - val_loss: 0.6197 - val_acc: 0.7864\n",
      "Epoch 79/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0992 - acc: 0.9919 - val_loss: 0.6059 - val_acc: 0.7857\n",
      "Epoch 80/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0927 - acc: 0.9961 - val_loss: 0.6113 - val_acc: 0.7827\n",
      "Epoch 81/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.0954 - acc: 0.9944 - val_loss: 0.6420 - val_acc: 0.7860\n",
      "Epoch 82/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0954 - acc: 0.9946 - val_loss: 0.6188 - val_acc: 0.7875\n",
      "Epoch 83/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0942 - acc: 0.9942 - val_loss: 0.6187 - val_acc: 0.7894\n",
      "Epoch 84/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0942 - acc: 0.9942 - val_loss: 0.6141 - val_acc: 0.7905\n",
      "Epoch 85/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0881 - acc: 0.9962 - val_loss: 0.6051 - val_acc: 0.7849\n",
      "Epoch 86/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0902 - acc: 0.9961 - val_loss: 0.6243 - val_acc: 0.7812\n",
      "Epoch 87/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0915 - acc: 0.9940 - val_loss: 0.6263 - val_acc: 0.7845\n",
      "Epoch 88/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0910 - acc: 0.9944 - val_loss: 0.6113 - val_acc: 0.7853\n",
      "Epoch 89/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.0936 - acc: 0.9942 - val_loss: 0.6221 - val_acc: 0.7842\n",
      "Epoch 90/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0912 - acc: 0.9955 - val_loss: 0.6135 - val_acc: 0.7853\n",
      "Epoch 91/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0906 - acc: 0.9942 - val_loss: 0.6185 - val_acc: 0.7868\n",
      "Epoch 92/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0898 - acc: 0.9942 - val_loss: 0.6093 - val_acc: 0.7860\n",
      "Epoch 93/100\n",
      "5330/5330 [==============================] - 1s 196us/step - loss: 0.0912 - acc: 0.9934 - val_loss: 0.6163 - val_acc: 0.7849\n",
      "Epoch 94/100\n",
      "5330/5330 [==============================] - 1s 198us/step - loss: 0.0852 - acc: 0.9968 - val_loss: 0.6247 - val_acc: 0.7819\n",
      "Epoch 95/100\n",
      "5330/5330 [==============================] - 2s 451us/step - loss: 0.0851 - acc: 0.9974 - val_loss: 0.6120 - val_acc: 0.7849\n",
      "Epoch 96/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0843 - acc: 0.9976 - val_loss: 0.6865 - val_acc: 0.7778\n",
      "Epoch 97/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0878 - acc: 0.9946 - val_loss: 0.6150 - val_acc: 0.7894\n",
      "Epoch 98/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0838 - acc: 0.9964 - val_loss: 0.6247 - val_acc: 0.7838\n",
      "Epoch 99/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0859 - acc: 0.9961 - val_loss: 0.6253 - val_acc: 0.7857\n",
      "Epoch 100/100\n",
      "5330/5330 [==============================] - 1s 197us/step - loss: 0.0844 - acc: 0.9961 - val_loss: 0.6080 - val_acc: 0.7864\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(nb_epoch):\n",
    "# print(\"\\n------------- Epoch %d ------------\" % (epoch+1))\n",
    "\n",
    "trModel = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          verbose=1,validation_data=(X_dev,y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2664/2664 [==============================] - 0s 91us/step\n",
      "2668/2668 [==============================] - 0s 83us/step\n",
      "Val-Accuracy: 78.64% (loss: 0.6080)\n",
      "Test-Accuracy: 78.67% (loss: 0.6298)\n"
     ]
    }
   ],
   "source": [
    "#Use Keras to compute the loss and the accuracy\n",
    "dev_loss, dev_accuracy = model.evaluate(X_dev, y_dev, verbose=1)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "\n",
    "print(\"Val-Accuracy: %.2f%% (loss: %.4f)\" % (dev_accuracy*100, dev_loss))\n",
    "print(\"Test-Accuracy: %.2f%% (loss: %.4f)\" % (test_accuracy*100, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Loss Curves')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Loss and Validation Loss\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(trModel.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(trModel.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accuracy Curves')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Accuracy and Validation Accuracy\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(trModel.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(trModel.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00612474], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/home/dl1/Arav/Sentence_classic/models/model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl1/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loadedModel = load_model(\"/home/dl1/Arav/Sentence_classic/models/model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n"
     ]
    }
   ],
   "source": [
    "result_data = pkl.load(gzip.open(\"/home/dl1/Arav/Sentence_classic/code/pkl/resultdata.pkl.gz\",\"rb\"))\n",
    "print(\"data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 41, 2610]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_X = sequence.pad_sequences(result_data, maxlen=max_sentence_len)\n",
    "# result_data.extend([0] * (59 - len(result_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    1,   41, 2610]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_y_pred = loadedModel.predict(result_X, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6230338], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_y_pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
